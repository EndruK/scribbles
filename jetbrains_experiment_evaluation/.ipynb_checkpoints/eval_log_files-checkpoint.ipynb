{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_log_files = \"/mnt/media/Experiments/SLP-Core\"\n",
    "path_to_orig_files = \"/mnt/media/Corpora/extern/Github_Java_Corpus/split\"\n",
    "variants = [\"01percent\", \"10percent\", \"50percent\"]\n",
    "modi = [\"training\", \"validation\"]\n",
    "\n",
    "delimiter = [\"<|CONTEXT|>\", \"<|MEASURE|>\", \"<|GENERATED_COMPLETIONS|>\", \"<|ALL_COMPLETIONS|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modus = 1\n",
    "variant = 0\n",
    "\n",
    "current_dataset = \"single_file_chopped_experiment\"\n",
    "current_experiment = \"smaller_\" + variants[variant]\n",
    "current_orig_file_path = os.path.join(path_to_orig_files, modi[modus] + \"_smaller_\" + variants[variant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read a file and return its content as a string\n",
    "[input]: full path to file\n",
    "[output]: content of the file as a string\n",
    "\"\"\"\n",
    "def read_file(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "\"\"\"\n",
    "split the given log file string into its components as given by the delimiter.\n",
    "delimiter have to follow the log file standard\n",
    "[input]: log file content as a string\n",
    "[output]: dict containing the content for each log file part with its corresponding delimiter as key\n",
    "\"\"\"\n",
    "def split_log_file(content):\n",
    "    result = {}\n",
    "    part = \"\"\n",
    "    lines = content.split(\"\\n\")\n",
    "    delim = lines[0].strip()\n",
    "    for i in range(1, len(lines)):\n",
    "        if lines[i].strip() in delimiter:\n",
    "            result[delim] = part\n",
    "            delim = lines[i].strip()\n",
    "            part = \"\"\n",
    "            continue\n",
    "        part += lines[i] + \"\\n\"\n",
    "    result[delim] = part\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "process the given completion log file string and put all generated completions in a list as tuple: \n",
    "completion-string:probability\n",
    "[input]: string holding the generated completions in each line\n",
    "[output]: list holding each completion:probability tuple as elements\n",
    "\"\"\"\n",
    "def process_generated_completions(generated_completion_text):\n",
    "    lines = generated_completion_text.split(\"\\n\")\n",
    "    result = []\n",
    "    lines = [line for line in lines if len(line) > 0]\n",
    "    for line in lines:\n",
    "        chunks = line.split(\" -- \")\n",
    "        completion = chunks[0]\n",
    "        probability = float(chunks[1][1:-1])\n",
    "        result.append([completion, probability])\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "process the given full completion string of intellij proposal mechanism (including the generated completions)\n",
    "[input]: string holding all completions in each line\n",
    "[output]: list holding triplet with the position, the completion string, and wheter its source is generated\n",
    "          or intellij\n",
    "\"\"\"\n",
    "def process_all_completions(all_completions_text):\n",
    "    # [position, text]\n",
    "    generated_text_overhead = [\"LookupElementBuilder: string=\", \"; handler=null\"]\n",
    "    result = []\n",
    "    lines = all_completions_text.split(\"\\n\")\n",
    "    lines = [line for line in lines if len(line) > 0]\n",
    "    for i, line in enumerate(lines):\n",
    "        # check if we have a generated completion\n",
    "        if any(check in line for check in generated_text_overhead):\n",
    "            tag = \"generated\"\n",
    "            element = line[len(generated_text_overhead[0]):-len(generated_text_overhead[1])]\n",
    "        else:\n",
    "            tag = \"intellij\"\n",
    "            element = line\n",
    "        result.append([i, element, tag])\n",
    "    return result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "get all files of the given path inside the subfolder and dataset folder\n",
    "[input]: path: path to log files\n",
    "         dataset: which dataset is used (01, 10, 50)\n",
    "         subfolder: which experiment is used (single file chopped, etc.)\n",
    "[output]: list holding all log files in the directory\n",
    "\"\"\"\n",
    "def get_log_files(path, dataset, subfolder):\n",
    "    new_path = os.path.join(path, subfolder)\n",
    "    new_path = os.path.join(new_path, dataset)\n",
    "    files = os.listdir(new_path)\n",
    "    return [os.path.join(new_path, file) for file in files if file.endswith(\".log\")]\n",
    "\n",
    "\"\"\"\n",
    "get all files of the verification set ending with .java\n",
    "[input]: path to the root of the verification set\n",
    "[output]: list holding the full paths for all files in the dataset\n",
    "\"\"\"\n",
    "def get_orig_files(path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".java\"):\n",
    "                result.append(os.path.join(root, file))\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "map the context of the log file to its original java file in the dataset\n",
    "[input]: path: the path to the logfile\n",
    "         orig_file_paths: list of all original files\n",
    "[output]: list of the files with the same name in the validation dataset\n",
    "\"\"\"\n",
    "def get_orig_file_path_from_log_file(path, orig_file_paths):\n",
    "    base_name = os.path.basename(path)\n",
    "    if base_name.endswith(\".log\"):\n",
    "        base_name = base_name[:-4]\n",
    "    multi = []\n",
    "    for p in orig_file_paths:\n",
    "        if base_name in p:\n",
    "            multi.append(p)\n",
    "    return multi\n",
    "\n",
    "\"\"\"\n",
    "get the true original file corresponding to the given context\n",
    "[input]: chopped_text: the context of the log file\n",
    "         file_list: the list of the files with the same name in the validation dataset\n",
    "[output]: the paths to the files with the same name in the validation dataset\n",
    "\"\"\"\n",
    "def get_orignal_file(chopped_text, file_list):\n",
    "    target = []\n",
    "    chopped_text_lines = chopped_text.split(\"\\n\")\n",
    "    for file in file_list:\n",
    "        with open(file, \"r\") as f:\n",
    "            text = f.read()\n",
    "        if text.startswith(\"\\n\".join(chopped_text_lines[:-2])):\n",
    "            target.append(file)\n",
    "    return target\n",
    "\n",
    "def get_next_token_in_orig_file():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st step: get all log files\n",
    "log_file_full_paths = get_log_files(path_to_log_files, current_dataset, current_experiment)\n",
    "\n",
    "# get all paths of all original files - have to find later that files we used for experiment\n",
    "all_original_file_paths = get_orig_files(current_orig_file_path)\n",
    "\n",
    "# 2nd step: get all corresponding original files\n",
    "files_with_same_name = []\n",
    "for log_file in log_file_full_paths:\n",
    "    files_with_same_name.append(get_orig_file_path_from_log_file(log_file, all_original_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "19\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(log_file_full_paths)):\n",
    "    if len(files_with_same_name[i]) != 1:\n",
    "        # TODO: hier weiter machen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orig_files = [get_orig_file_path_from_log_file(path, all_original_file_paths) for path in log_file_full_paths]\n",
    "#orig_files_cleaned = orig_files.copy()\n",
    "#for i, file in enumerate(orig_files):\n",
    "#    if len(file) > 1:\n",
    "#        base_name = os.path.basename(file[0]) + \".log\"\n",
    "#        p = path_to_log_files + \"/\" + current_experiment + \"/\" + current_dataset + \"/\" + base_name\n",
    "#        p_text = read_file(p)\n",
    "#        orig_files_cleaned[i] = get_orignal_file(p_text, file)\n",
    "#    else:\n",
    "#        orig_files_cleaned[i] = file\n",
    "#        \n",
    "\n",
    "#for i, item in enumerate(orig_files_cleaned):\n",
    "#    if len(item) != 1:\n",
    "#        print(log_file_full_paths[i])\n",
    "#        #print(orig_files[i])\n",
    "#        print(len(orig_files_cleaned[i]))\n",
    "\n",
    "#orig_files = [file[0] for file in orig_files]\n",
    "#orig_file_text = \"\\n\".join(orig_files)\n",
    "#with open(\"original_file_paths\", \"w\") as f:\n",
    "#    f.write(orig_file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/media/Corpora/extern/Github_Java_Corpus/split/validation_smaller_01percent/arquillian_deprecated/protocols/jmx/src/test/java/org/jboss/arquillian/jmx/JMXTestRunnerTestCase.java']\n"
     ]
    }
   ],
   "source": [
    "#log_files = get_log_files(path_to_log_files, current_dataset, current_experiment)\n",
    "#print(log_files)\n",
    "orig_files = get_orig_files(current_orig_file_path)\n",
    "hans = get_orig_file_path_from_log_file(\"/mnt/media/Experiments/SLP-Core/smaller_50percent/single_file_chopped_experiment/JMXTestRunnerTestCase.java.log\", orig_files)\n",
    "print(hans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 'execute', 'generated']\n",
      "[1, 'getResults', 'generated']\n",
      "[2, 'getTestCase', 'generated']\n",
      "[3, 'mask', 'generated']\n",
      "[4, 'start', 'generated']\n",
      "[5, '.arg', 'intellij']\n",
      "[6, '.cast', 'intellij']\n",
      "[7, '.field', 'intellij']\n",
      "[8, '.par', 'intellij']\n",
      "[9, '.try', 'intellij']\n",
      "[10, '.var', 'intellij']\n"
     ]
    }
   ],
   "source": [
    "test = \"/mnt/media/Experiments/SLP-Core/smaller_50percent/single_file_chopped_experiment/JMXTestRunnerTestCase.java.log\"\n",
    "text = read_file(test)\n",
    "bla = split_log_file(text)\n",
    "foo = process_generated_completions(bla[delimiter[2]])\n",
    "baz = process_all_completions(bla[delimiter[3]])\n",
    "for item in baz:\n",
    "    print(item)\n",
    "    \n",
    "#print(foo)\n",
    "#for key, item in bla.items():\n",
    "#    print(key)\n",
    "#    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
